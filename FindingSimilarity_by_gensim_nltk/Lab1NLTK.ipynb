{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Sentences tokenization\n",
    "- Dividing paragraph to sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_paragraph = \"Move Forward now represents the bulk of the parliamentary opposition \\\n",
    "but will retain significant political clout having won the majority of seats in \\\n",
    "and around the capital Bangkok and taken over key urban centres and some conservative strongholds. \\\n",
    "The Harvard-educated Pita, 43, was twice denied by parliament in his efforts to become prime minister \\\n",
    "as military-appointed senators closed ranks to stop Move Forward, some over its controversial plans \\\n",
    "to amend a law that insulates the monarchy from criticism.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize_list = nltk.sent_tokenize(text_paragraph)\n",
    "\n",
    "len(sent_tokenize_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Word tokenization\n",
    "- Segment sentences to the list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of words :\n",
      "\n",
      "['Let', \"'s\", 'see', 'how', 'it', \"'s\", 'working', '.']\n",
      "\n",
      "number of words : 8\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Let's see how it's working.\"\n",
    "list_of_words = nltk.word_tokenize(sentence)\n",
    "\n",
    "print(F\"List of words :\\n\\n{ list_of_words }\\n\\nnumber of words : { len(list_of_words) }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Let's\", 'see', 'how', \"it's\", 'working']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This method will include apostrophe\n",
    "nltk.RegexpTokenizer(\"[\\w']+\").tokenize(sentence)\n",
    "\n",
    "# \"TabTokenizer\" is a word tokenizer by tab or \\t\n",
    "# \"LineTokenizer\" is a word tokenizer by line or \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'coool',\n",
       " '#Dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<-',\n",
       " '->']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"This is a coool #Dummysmiley: :-) :-P <3 and some arrows <- ->\"\n",
    "\n",
    "# This method will count hashtag as a word\n",
    "nltk.TweetTokenizer().tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "- Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FreqDist with 15 samples and 27 outcomes>\n",
      "The most frequency word : [(' ', 4), ('e', 3)]\n"
     ]
    }
   ],
   "source": [
    "fcount = nltk.FreqDist(sentence)\n",
    "\n",
    "print( fcount )\n",
    "print( F\"The most frequency word : { fcount.most_common(2) }\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unrequire word in english\n",
    "stop_word = set( nltk.corpus.stopwords.words(\"english\") )\n",
    "\n",
    "# stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before using stop word :\n",
      "['Let', \"'s\", 'see', 'how', 'it', \"'s\", 'working', '.']\n",
      "\n",
      "After using stop word :\n",
      "['Let', \"'s\", 'see', \"'s\", 'working', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter_sen = []\n",
    "# for w in nltk.word_tokenize(sentence) :\n",
    "#     if w not in stop_word :\n",
    "#         filter_sen.append(w)\n",
    "\n",
    "# or use this method\n",
    "\n",
    "filter_sen = [ w for w in nltk.word_tokenize(sentence) if w not in stop_word ]\n",
    "\n",
    "print( F\"Before using stop word :\\n{nltk.word_tokenize(sentence)}\\n\" )\n",
    "print( F\"After using stop word :\\n{filter_sen}\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Normalization\n",
    "- STEM\n",
    "- LEMMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Porter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love  :  love\n",
      "loved  :  love\n",
      "loving  :  love\n",
      "loves  :  love\n",
      " \n",
      "Programmers  :  programm\n",
      "program  :  program\n",
      "with  :  with\n",
      "programming  :  program\n",
      "languages  :  languag\n",
      " \n",
      "fly  :  fli\n",
      "flying  :  fli\n",
      "corpus  :  corpu\n",
      "corpura  :  corpura\n",
      "study  :  studi\n",
      "studying  :  studi\n",
      "studies  :  studi\n"
     ]
    }
   ],
   "source": [
    "ps = nltk.stem.PorterStemmer()\n",
    "\n",
    "sentence1 = ['love','loved','loving','loves']\n",
    "sentence2 = nltk.word_tokenize(\"Programmers program with programming languages\")\n",
    "sentence3 = ['fly','flying','corpus','corpura','study','studying','studies']\n",
    "\n",
    "for w in sentence1 :\n",
    "    print( w,\" : \",ps.stem(w) )\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "for w in sentence2 :\n",
    "    print( w,\" : \",ps.stem(w) )\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "for w in sentence3 :\n",
    "    print( w,\" : \",ps.stem(w) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Snowball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love  :  love\n",
      "loved  :  love\n",
      "loving  :  love\n",
      "loves  :  love\n",
      " \n",
      "Programmers  :  programm\n",
      "program  :  program\n",
      "with  :  with\n",
      "programming  :  program\n",
      "languages  :  languag\n",
      " \n",
      "fly  :  fli\n",
      "flying  :  fli\n",
      "corpus  :  corpus\n",
      "corpura  :  corpura\n",
      "study  :  studi\n",
      "studying  :  studi\n",
      "studies  :  studi\n"
     ]
    }
   ],
   "source": [
    "snow = nltk.stem.SnowballStemmer(\"english\")\n",
    "\n",
    "for w in sentence1 :\n",
    "    print( w,\" : \",snow.stem(w) )\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "for w in sentence2 :\n",
    "    print( w,\" : \",snow.stem(w) )\n",
    "\n",
    "print(\" \")\n",
    "\n",
    "for w in sentence3 :\n",
    "    print( w,\" : \",snow.stem(w) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation\n",
    "Named entity tagging\n",
    "\n",
    "- Penn treebank POS tagset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Let', 'VB')]\n",
      "[(\"'s\", 'POS')]\n",
      "[('see', 'VB')]\n",
      "[('how', 'WRB')]\n",
      "[('it', 'PRP')]\n",
      "[(\"'s\", 'POS')]\n",
      "[('working', 'VBG')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# grammar classification\n",
    "# for more details : https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "for sen in nltk.word_tokenize(sentence) :\n",
    "    print( nltk.pos_tag( nltk.word_tokenize( sen ) ) )\n",
    "\n",
    "# or nltk.tokenize.PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic\n",
    "Giving a meaning of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('bank.n.01'),\n",
       " Synset('depository_financial_institution.n.01'),\n",
       " Synset('bank.n.03'),\n",
       " Synset('bank.n.04'),\n",
       " Synset('bank.n.05'),\n",
       " Synset('bank.n.06'),\n",
       " Synset('bank.n.07'),\n",
       " Synset('savings_bank.n.02'),\n",
       " Synset('bank.n.09'),\n",
       " Synset('bank.n.10'),\n",
       " Synset('bank.v.01'),\n",
       " Synset('bank.v.02'),\n",
       " Synset('bank.v.03'),\n",
       " Synset('bank.v.04'),\n",
       " Synset('bank.v.05'),\n",
       " Synset('deposit.v.02'),\n",
       " Synset('bank.v.07'),\n",
       " Synset('trust.v.01')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn = nltk.corpus.wordnet.synsets('bank')\n",
    "\n",
    "syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['they pulled the canoe up on the bank',\n",
       " 'he sat on the bank of the river and watched the currents']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sloping land (especially the slope beside a body of water)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonym : ['good', 'good', 'goodness', 'good', 'goodness', 'commodity', 'trade_good', 'good', 'good', 'full', 'good', 'good', 'estimable', 'good', 'honorable', 'respectable', 'beneficial', 'good', 'good', 'good', 'just', 'upright', 'adept', 'expert', 'good', 'practiced', 'proficient', 'skillful', 'skilful', 'good', 'dear', 'good', 'near', 'dependable', 'good', 'safe', 'secure', 'good', 'right', 'ripe', 'good', 'well', 'effective', 'good', 'in_effect', 'in_force', 'good', 'good', 'serious', 'good', 'sound', 'good', 'salutary', 'good', 'honest', 'good', 'undecomposed', 'unspoiled', 'unspoilt', 'good', 'well', 'good', 'thoroughly', 'soundly', 'good']\n",
      "Antonym : ['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n"
     ]
    }
   ],
   "source": [
    "myword = 'good'\n",
    "\n",
    "syn = []\n",
    "ant = []\n",
    "\n",
    "for synset in nltk.corpus.wordnet.synsets(myword) :\n",
    "    for lemm in synset.lemmas():\n",
    "        syn.append(lemm.name())\n",
    "        \n",
    "        if lemm.antonyms() :\n",
    "            ant.append( lemm.antonyms()[0].name() )\n",
    "\n",
    "print( F\"Synonym : {syn}\\nAntonym : {ant}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To chunk the given list of tagged token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_tree = nltk.ne_chunk( nltk.pos_tag(text_paragraph.split()), binary=True )\n",
    "\n",
    "# binary = True --> identify NE\n",
    "# binary = False --> identify NE categories\n",
    "\n",
    "# print( parse_tree )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Tree('NE', [('Move', 'NNP'), ('Forward', 'NNP')]), Tree('NE', [('Bangkok', 'NNP')]), Tree('NE', [('Move', 'NNP')])]\n"
     ]
    }
   ],
   "source": [
    "# name_entities = list()\n",
    "# for t in parse_tree.subtrees():\n",
    "#     if t.label() == 'NE' :\n",
    "#         name_entities.append(t)\n",
    "\n",
    "# or\n",
    "\n",
    "name_entities = [ t for t in parse_tree.subtrees() if t.label() == 'NE' ]\n",
    "\n",
    "print(name_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
